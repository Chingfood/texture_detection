# -*- coding: utf-8 -*-
"""classifier_accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13VRTDDfQ6_2qDbze8W-7-72oTlYXXcFj
"""



import pickle
import numpy as np
import os

import time

import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsClassifier

from sklearn.cluster import MiniBatchKMeans

path = ""

with open(os.path.join(path,"label2num.pkl"), "rb") as fp:   # Unpickling
    label2num = pickle.load(fp)
    
with open(os.path.join(path,"num2label.pkl"), "rb") as fp:   # Unpickling
    num2label = pickle.load(fp)

with open(os.path.join(path,"train_feature_histogram_list.pkl"), "rb") as fp:   #Pickling
    train_feature_histogram_list=pickle.load(fp)

with open(os.path.join(path,"validate_feature.pkl"), "rb") as fp:   # Unpickling
    validate_feature = pickle.load(fp)

with open(os.path.join(path,"validate_label.pkl"), "rb") as fp:   # Unpickling
    validate_label = pickle.load(fp)

def texton_histogram_feature_construct(feature,kmeans_model):
    points_np = np.concatenate(feature, axis=0)
    texton_label = kmeans_model.predict(points_np)
    number_of_clusters = kmeans_model.get_params()['n_clusters']
  
    texton_space_feature = []
    start = 0
    for i in range(len(feature)):
        end = feature[i].shape[0]+start
        texton_space_feature.append(texton_label[start:end])
        start = end
  
    feature_texto_histogram = []
    for i in range(len(texton_space_feature)):
        unique, counts = np.unique(texton_space_feature[i], return_counts=True)
        histo_dict = dict(zip(unique, counts))
      
        histo_np = np.zeros(number_of_clusters).astype(np.float32)

        for j in range(number_of_clusters):
            if j in histo_dict:
                histo_np[j] = histo_dict[j]/counts.sum()
        feature_texto_histogram.append(np.clip(histo_np, 1e-7, np.inf))
    return feature_texto_histogram

def distance_matrix_construct(query_feature,target_feature):
    """return the distance between each samples"""
    def chi2_distance(list_1, list_2):
        d = 0.5*( (list_1-list_2)**2 / (list_1+list_2) ).sum()
        return d

    query_dist = np.zeros([len(query_feature),len(target_feature)])
    for i in range(query_dist.shape[0]):
        for j in range(query_dist.shape[1]):
            query_dist[i,j] = chi2_distance(query_feature[i],target_feature[j])
    return query_dist

def true_predicted_label_mat(true_label, label_predicted, num_of_classes=47):
  #construct a matrix that store the value of true labels vs. predicted labels table
  #axis0 is true label, axis1 is the predicted label

    def list_indices(list_1, element):
        indices = [i for i,x in enumerate(list_1) if x == element]
        return indices

    u = np.unique(true_label)
    label_mat = np.zeros([num_of_classes,num_of_classes])

    for i in range(u.shape[0]):
        if i != u[i]: print("something is wrong")
        indices = list_indices(true_label, u[i])
        for index in indices:
              label_mat[u[i],label_predicted[index]] += 1
    return label_mat

def true_predicted_label_prob_mat(true_label, label_predicted_probability, num_of_classes=47):
  #construct a matrix that store the value of true labels vs. predicted labels table
  #axis0 is true label, axis1 is the predicted label
  
    def list_indices(list_1, element):
        indices = [i for i,x in enumerate(list_1) if x == element]
        return indices

    u = np.unique(true_label)
    label_prob_mat = np.zeros([num_of_classes,num_of_classes])

    for i in range(u.shape[0]):
        if i != u[i]: print("something is wrong")
        indices = list_indices(true_label, u[i])
        for index in indices:  
            label_prob_mat[u[i]] += label_predicted_prob[index]
    return label_prob_mat

def confusion_mat_generating(labelXlabel_mat):
    label_confusion_mat = np.zeros([labelXlabel_mat.shape[0],2,2])

    """
    True Positive [0,0]
    False Negative [0,1]
    False Positive [1,0]
    True Negative [1,1]
    """

    for i in range(labelXlabel_mat.shape[0]):
        label_confusion_mat[i][0,0] = labelXlabel_mat[i,i]
        label_confusion_mat[i][0,1] = labelXlabel_mat[i,:].sum() - labelXlabel_mat[i,i] 
        label_confusion_mat[i][1,0] = labelXlabel_mat[:,i].sum() - labelXlabel_mat[i,i]
        label_confusion_mat[i][1,1] = labelXlabel_mat.sum() - labelXlabel_mat[i,:].sum() - labelXlabel_mat[:,i].sum() + labelXlabel_mat[i,i]
    
    return label_confusion_mat

def accuracy(confusion_mat):
    """
    return accuracy and precision
    
    confusion_mat with shape number_of_classes *2 *2
    
    return a matrix: accu_mat, with shape number_of_classes*2
    accu_mat[:,0] is precision
    accu_mat[:,1] is recall
    """
    if len(confusion_mat.shape) == 3:
        accu_mat = np.zeros([confusion_mat.shape[0],2])
        for i in range(confusion_mat.shape[0]): 
            accu_mat[i,0] = confusion_mat[i,0,0]/(confusion_mat[i,0,0] + confusion_mat[i,1,0])
            accu_mat[i,1] = confusion_mat[i,0,0]/(confusion_mat[i,0,0] + confusion_mat[i,0,1])
    
    elif len(confusion_mat.shape) == 2:
        accu_mat = np.zeros([2])
        accu_mat[0] = confusion_mat[0,0]/(confusion_mat[0,0] + confusion_mat[1,0])
        accu_mat[1] = confusion_mat[0,0]/(confusion_mat[0,0] + confusion_mat[0,1])
    return accu_mat

def highest_value_diagonal_removed(labelXlabel_mat):
    mat = labelXlabel_mat.copy()
    np.fill_diagonal(mat, -np.inf)
    top_5_value_index = mat.argsort(axis=None)[-5:][::-1]
    top_5_value_coord = np.unravel_index(top_5_value_index, mat.shape)
    return top_5_value_coord

num_of_classes=47
accuracy_list = []
accuracy_lab_list = []

for scale in range(1,11):

    number_of_clusters = num_of_classes*scale

    with open(os.path.join(path,"batch_kmeans_model_"+str(number_of_clusters)+".pkl"), "rb") as fp:   #Pickling
        kmeans = pickle.load(fp)

    validate_feature_texto_histogram = texton_histogram_feature_construct(validate_feature, kmeans)

    query_distance = distance_matrix_construct(validate_feature_texto_histogram,train_feature_histogram_list[scale-1])

    with open(os.path.join(path,"knn_texton_"+str((scale)*num_of_classes)+".pkl"), "rb") as fp:   #Pickling
        neigh = pickle.load(fp)

    label_predicted_prob = neigh.predict_proba(query_distance)
    label_predicted = label_predicted_prob.argmax(axis=1)

    validate_labellabel_mat = true_predicted_label_mat(validate_label, label_predicted)
    validate_labellabel_prob_mat = true_predicted_label_prob_mat(validate_label,label_predicted_prob)

    confusion_mat_label = confusion_mat_generating(validate_labellabel_mat)
    accuracy_mat_label = accuracy(confusion_mat_label)
    average_confusion_mat_label = confusion_mat_label.mean(axis=0)
    average_accuracy_mat_label = accuracy_mat_label.mean(axis=0)
    accuracy_list.append(average_accuracy_mat_label)

    confusion_mat_label_prob = confusion_mat_generating(validate_labellabel_prob_mat)
    accuracy_mat_label_prob = accuracy(confusion_mat_label_prob)
    average_confusion_mat_label_prob = confusion_mat_label_prob.mean(axis=0)
    average_accuracy_mat_label_prob = accuracy_mat_label_prob.mean(axis=0)
    accuracy_lab_list.append(average_accuracy_mat_label_prob)

accuracy_list_np = np.array(accuracy_list)
accuracy_lab_list_np = np.array(accuracy_lab_list)

plt.figure(figsize=(10, 10))
t = np.arange(1,11)*47
plt.plot(t, accuracy_list_np[:,0], 'r*',label='precision', ms=15) #line plot
plt.plot(t, accuracy_list_np[:,1], 'b^',label='recall', ms=15) #scatter plot
plt.xlabel('number of textons',fontsize = 20)
plt.ylabel('accuracy', fontsize = 20)
plt.title(f'Precision and Recall vs. number of textons',fontsize=30)
plt.legend(loc =2, fontsize=20)
plt.show()

with open(os.path.join(path,"test_feature.pkl"), "rb") as fp:   # Unpickling
    test_feature = pickle.load(fp)

with open(os.path.join(path,"test_label.pkl"), "rb") as fp:   # Unpickling
    test_label = pickle.load(fp)

num_of_classes=47

scale = 10

number_of_clusters = num_of_classes*scale

with open(os.path.join(path,"batch_kmeans_model_"+str(number_of_clusters)+".pkl"), "rb") as fp:   #Pickling
    kmeans = pickle.load(fp)

test_feature_texto_histogram = texton_histogram_feature_construct(test_feature, kmeans)

query_distance = distance_matrix_construct(test_feature_texto_histogram,train_feature_histogram_list[scale-1])

with open(os.path.join(path,"knn_texton_"+str((scale)*num_of_classes)+".pkl"), "rb") as fp:   #Pickling
    neigh = pickle.load(fp)

label_predicted_prob = neigh.predict_proba(query_distance)
label_predicted = label_predicted_prob.argmax(axis=1)

test_labellabel_mat = true_predicted_label_mat(test_label, label_predicted)

confusion_mat_label = confusion_mat_generating(test_labellabel_mat)
accuracy_mat_label = accuracy(confusion_mat_label)
average_confusion_mat_label = confusion_mat_label.mean(axis=0)
average_accuracy_mat_label = accuracy_mat_label.mean(axis=0)

top5coord = highest_value_diagonal_removed(test_labellabel_mat)



plt.figure(figsize=(20, 20))
plt.title("True label vs. Predicted label",fontsize=30)
plt.xlabel("Predicted Label",fontsize = 20)
plt.ylabel("True Label",fontsize = 20)
plt.imshow(test_labellabel_mat)
plt.xticks(range(47),labels=list(label2num), rotation='vertical')
plt.yticks(range(47),labels=list(label2num))
plt.plot(top5coord[1][0], top5coord[0][0], 'o', ms=50, markeredgecolor='w', markerfacecolor="none",label='most confused',)
plt.plot(top5coord[1][1:], top5coord[0][1:], 'o', ms=50, markeredgecolor='r', markerfacecolor="none", label='top 5 confused')
plt.legend(loc =0, fontsize=20)
plt.colorbar()
plt.show()

plt.figure(figsize=(10, 10))
t = np.arange(47)
plt.plot(t, accuracy_mat_label[:,0], 'r*',label='Precision', ms=15) #line plot
plt.plot(t, accuracy_mat_label[:,1], 'b^',label='Recall', ms=15) #scatter plot
plt.hlines(average_accuracy_mat_label[0], xmin=0, xmax=47, colors='r', linestyles='dashed',label="Average Precision")
plt.hlines(average_accuracy_mat_label[1], xmin=0, xmax=47, colors='b', linestyles='dashed',label="Average Recall")
plt.xlabel('labels',fontsize = 20)
plt.ylabel('accuracy', fontsize = 20)
plt.title(f'Precision and Recall vs. different classes',fontsize=30)
plt.legend(loc =0, fontsize=10)
plt.xticks(range(47),labels=list(label2num), rotation='vertical')

plt.show()

average_confusion_mat_label

average_accuracy_mat_label

# num_of_classes=47
# accuracy_list = []
# scale = 1

# number_of_clusters = num_of_classes*scale
# with open(os.path.join(path,"batch_kmeans_model_"+str(number_of_clusters)+".pkl"), "rb") as fp:   #Pickling
#   kmeans = pickle.load(fp)

# validate_feature_texto_histogram = texton_histogram_feature_construct(validate_feature, kmeans)
# query_distance = distance_matrix_construct(validate_feature_texto_histogram,train_feature_histogram_list[scale-1])
# with open(os.path.join(path,"knn_texton_"+str((scale)*num_of_classes)+".pkl"), "rb") as fp:   #Pickling
#   neigh = pickle.load(fp)
# label_predicted_prob = neigh.predict_proba(query_distance)
# label_predicted = label_predicted_prob.argmax(axis=1)
# validate_labellabel_mat = true_predicted_label_mat(validate_label, label_predicted)
# validate_labellabel_prob_mat = true_predicted_label_prob_mat(validate_label,label_predicted_prob)
# confusion_mat_label = confusion_mat_generating(validate_labellabel_mat)
# accuracy_mat_label = accuracy(confusion_mat_label)
# combined_confusion_mat_label = confusion_mat_label.sum(axis=0)
# combined_accuracy_mat_label = accuracy(combined_confusion_mat_label)

# confusion_mat_label_prob = confusion_mat_generating(validate_labellabel_prob_mat)
# accuracy_mat_label_prob = accuracy(confusion_mat_label_prob)
# combined_confusion_mat_label_prob = confusion_mat_label_prob.sum(axis=0)
# combined_accuracy_mat_label_prob = accuracy(combined_confusion_mat_label_prob)